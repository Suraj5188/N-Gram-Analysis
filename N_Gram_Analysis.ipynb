{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Time Series Algorithms: Explanation and Types\n",
        "\n",
        "A time series is a sequence of data points collected or recorded at regular time intervals (e.g., hourly, daily, monthly). Time series analysis involves methods to analyze this temporal data to extract meaningful statistics and characteristics, while time series forecasting uses models to predict future values based on previously observed values.\n",
        "\n",
        "## Main Types of Time Series Algorithms\n",
        "\n",
        "### 1. **Traditional Statistical Methods**\n",
        "   - **ARIMA (AutoRegressive Integrated Moving Average)**\n",
        "     - Combines autoregression (AR), differencing (I), and moving average (MA)\n",
        "     - Good for stationary series (where statistical properties don't change over time)\n",
        "     - Variations: SARIMA (seasonal ARIMA), ARIMAX (with exogenous variables)\n",
        "\n",
        "   - **Exponential Smoothing**\n",
        "     - Weighted averages of past observations, with weights decaying exponentially\n",
        "     - Types: Simple, Double (Holt's), Triple (Holt-Winters for seasonality)\n",
        "     - Good for data with trend and/or seasonality\n",
        "\n",
        "   - **Vector Autoregression (VAR)**\n",
        "     - Generalization of AR for multiple time series variables\n",
        "     - Captures linear interdependencies among multivariate time series\n",
        "\n",
        "### 2. **Machine Learning Approaches**\n",
        "   - **Random Forests & Gradient Boosting (XGBoost, LightGBM)**\n",
        "     - Can handle time series by engineering temporal features\n",
        "     - Good for capturing complex nonlinear relationships\n",
        "\n",
        "   - **Support Vector Regression (SVR)**\n",
        "     - Uses kernel tricks to model nonlinear relationships\n",
        "     - Requires careful feature engineering for time series\n",
        "\n",
        "   - **Neural Networks**\n",
        "     - Feedforward NNs with time-based features\n",
        "     - Often outperformed by specialized architectures for sequential data\n",
        "\n",
        "### 3. **Deep Learning Methods**\n",
        "   - **RNNs (Recurrent Neural Networks)**\n",
        "     - Designed for sequential data with internal memory\n",
        "     - Variants: LSTM (Long Short-Term Memory), GRU (Gated Recurrent Unit)\n",
        "     - Effective for long-term dependencies\n",
        "\n",
        "   - **TCN (Temporal Convolutional Networks)**\n",
        "     - Uses causal convolutions for time series\n",
        "     - Can capture long-range dependencies like RNNs but with parallel processing\n",
        "\n",
        "   - **Transformers**\n",
        "     - Attention mechanisms to weigh importance of different time steps\n",
        "     - Variants: Informer, Autoformer (designed for long sequence forecasting)\n",
        "\n",
        "### 4. **Hybrid Models**\n",
        "   - Combinations like ARIMA + Neural Networks\n",
        "   - Prophet (by Facebook) - additive model with nonlinear trends + seasonality + holidays\n",
        "   - N-BEATS - neural basis expansion for interpretable time series forecasting\n",
        "\n",
        "### 5. **Ensemble Methods**\n",
        "   - Combining predictions from multiple models\n",
        "   - Stacking models (using predictions as inputs to a meta-model)\n",
        "\n",
        "## Choosing the Right Algorithm\n",
        "\n",
        "The best algorithm depends on:\n",
        "- Data characteristics (seasonality, trend, noise)\n",
        "- Data size and frequency\n",
        "- Forecast horizon (short-term vs long-term)\n",
        "- Computational resources\n",
        "- Need for interpretability vs pure accuracy\n"
      ],
      "metadata": {
        "id": "DMMuGldTtcTa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyXvcl-XtXpE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **N-gram Analysis: Advantages and Disadvantages**\n",
        "\n",
        "N-grams are widely used in NLP and text processing, but they come with trade-offs. Below is a detailed breakdown of their **advantages** and **disadvantages**.\n",
        "\n",
        "---\n",
        "\n",
        "## **‚úÖ Advantages of N-grams**\n",
        "### **1. Simple & Easy to Implement**\n",
        "   - Requires minimal computational power compared to deep learning models.\n",
        "   - Works well for small datasets where neural networks may overfit.\n",
        "\n",
        "### **2. Fast Execution**\n",
        "   - N-gram models are lightweight and efficient for real-time applications (e.g., autocomplete, spell-checking).\n",
        "\n",
        "### **3. Interpretable**\n",
        "   - Unlike deep learning models (e.g., Transformers), N-grams are transparent‚Äîwe can see which word sequences are most frequent.\n",
        "\n",
        "### **4. Works Well for Short Contexts**\n",
        "   - Effective for tasks where local word order matters (e.g., spam detection, sentiment analysis).\n",
        "\n",
        "### **5. No Training Required (for Basic Counting)**\n",
        "   - Unlike machine learning models, basic N-gram frequency counting does not require training.\n",
        "\n",
        "### **6. Useful for Feature Extraction**\n",
        "   - Can be used as features in machine learning models (e.g., TF-IDF + N-grams for text classification).\n",
        "\n",
        "---\n",
        "\n",
        "## **‚ùå Disadvantages of N-grams**\n",
        "### **1. Suffers from the \"Curse of Dimensionality\"**\n",
        "   - As `N` increases, the number of possible N-grams grows exponentially, requiring large memory.\n",
        "   - Example: A 5-gram model over 10,000 words needs **10,000‚Åµ = 10¬≤‚Å∞** entries (impractical).\n",
        "\n",
        "### **2. Data Sparsity (Zero-Frequency Problem)**\n",
        "   - Rare N-grams may never appear in training data, leading to zero probabilities.\n",
        "   - Requires **smoothing techniques** (e.g., Laplace, Kneser-Ney) to handle unseen N-grams.\n",
        "\n",
        "### **3. Limited Context Window**\n",
        "   - Only captures **local dependencies** (e.g., bigrams only consider pairs).\n",
        "   - Struggles with **long-range dependencies** (e.g., \"The cat, which was very hungry, **meowed**\").\n",
        "\n",
        "### **4. No Semantic Understanding**\n",
        "   - Treats words as discrete symbols without understanding meaning.\n",
        "   - Example: \"happy\" and \"joyful\" are treated as completely different, even if semantically similar.\n",
        "\n",
        "### **5. Poor Generalization**\n",
        "   - If a phrase was not seen in training, the model fails to predict it.\n",
        "   - Example: If \"machine learning\" was never seen, the model cannot suggest it.\n",
        "\n",
        "### **6. Struggles with Rare Words**\n",
        "   - Misspelled words, slang, or domain-specific terms may break predictions.\n",
        "\n",
        "---\n",
        "\n",
        "## **Comparison Table: N-grams vs. Modern NLP Models**\n",
        "| Feature | N-grams | Neural Models (LSTMs, Transformers) |\n",
        "|---------|---------|-------------------------------------|\n",
        "| **Context Handling** | Limited (local) | Long-range dependencies |\n",
        "| **Memory Usage** | High for large `N` | More efficient with embeddings |\n",
        "| **Training Time** | Fast (counting) | Slow (requires backpropagation) |\n",
        "| **Generalization** | Poor (needs smoothing) | Better (embeddings capture semantics) |\n",
        "| **Interpretability** | High (explicit counts) | Low (black-box models) |\n",
        "| **Use Case** | Simple tasks (autocomplete) | Complex tasks (translation, summarization) |\n",
        "\n",
        "---\n",
        "\n",
        "## **When to Use N-grams?**\n",
        "‚úî **Small datasets** (where deep learning would overfit)  \n",
        "‚úî **Low-resource environments** (e.g., mobile apps, embedded systems)  \n",
        "‚úî **Explainability matters** (e.g., detecting common phrases in customer reviews)  \n",
        "‚úî **Real-time applications** (e.g., search query suggestions)  \n",
        "\n",
        "## **When to Avoid N-grams?**\n",
        "‚úñ **Large-scale language modeling** (use Transformers instead)  \n",
        "‚úñ **Tasks requiring semantic understanding** (e.g., paraphrasing)  \n",
        "‚úñ **Handling rare/unknown words** (neural models generalize better)  \n",
        "\n",
        "---\n",
        "\n",
        "## **Final Verdict**\n",
        "N-grams are **simple, fast, and interpretable**, but they struggle with **sparsity, long-range dependencies, and semantic meaning**. For modern NLP tasks, **neural language models (LSTMs, Transformers)** are often better, but N-grams remain useful in lightweight applications.\n",
        "\n",
        "Would you like a **hybrid approach** (e.g., N-grams + Neural Networks) or **advanced smoothing techniques**? üöÄ"
      ],
      "metadata": {
        "id": "asnnFKCstv8w"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ehnbr6xKtyyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **N-gram Analysis: Theoretical Foundations & Detailed Examples**\n",
        "\n",
        "## **1. Theoretical Foundations of N-grams**\n",
        "\n",
        "### **1.1 Definition & Mathematical Formulation**\n",
        "An **N-gram** is a contiguous sequence of *N* items (words, characters, or symbols) from a given text.\n",
        "\n",
        "- Given a sequence of words:  \n",
        "  \\( S = (w_1, w_2, w_3, \\dots, w_T) \\)  \n",
        "- An N-gram of length *n* is a subsequence:  \n",
        "  \\( (w_i, w_{i+1}, \\dots, w_{i+n-1}) \\)  \n",
        "\n",
        "### **1.2 Probability Estimation**\n",
        "N-grams are used in **statistical language modeling** to predict the next word in a sequence using the **Markov assumption** (only the previous *N-1* words matter).\n",
        "\n",
        "- **Unigram (1-gram)**:  \n",
        "  \\( P(w_i) = \\frac{\\text{Count}(w_i)}{\\text{Total words}} \\)  \n",
        "\n",
        "- **Bigram (2-gram)**:  \n",
        "  \\( P(w_i | w_{i-1}) = \\frac{\\text{Count}(w_{i-1}, w_i)}{\\text{Count}(w_{i-1})} \\)  \n",
        "\n",
        "- **N-gram (General Case)**:  \n",
        "  \\( P(w_i | w_{i-n+1}, \\dots, w_{i-1}) = \\frac{\\text{Count}(w_{i-n+1}, \\dots, w_i)}{\\text{Count}(w_{i-n+1}, \\dots, w_{i-1})} \\)  \n",
        "\n",
        "### **1.3 Smoothing Techniques (Handling Zero Probabilities)**\n",
        "Since unseen N-grams lead to zero probabilities, smoothing methods adjust counts:\n",
        "- **Laplace (Add-1) Smoothing**:  \n",
        "  \\( P(w_i | w_{i-1}) = \\frac{\\text{Count}(w_{i-1}, w_i) + 1}{\\text{Count}(w_{i-1}) + V} \\)  \n",
        "  (where \\( V \\) = vocabulary size)\n",
        "\n",
        "- **Kneser-Ney Smoothing**:  \n",
        "  More advanced, considers **continuation probability** of words.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Detailed Example: Building a Bigram Model from Scratch**\n",
        "\n",
        "### **2.1 Corpus Preprocessing**\n",
        "Consider the following corpus:\n",
        "```\n",
        "\"The cat chased the dog. The dog barked at the cat.\"\n",
        "```\n",
        "\n",
        "**Step 1: Tokenization & Lowercasing**\n",
        "```python\n",
        "import re\n",
        "from collections import defaultdict\n",
        "\n",
        "text = \"The cat chased the dog. The dog barked at the cat.\"\n",
        "tokens = re.findall(r'\\w+', text.lower())\n",
        "print(tokens)\n",
        "```\n",
        "**Output**:\n",
        "```\n",
        "['the', 'cat', 'chased', 'the', 'dog', 'the', 'dog', 'barked', 'at', 'the', 'cat']\n",
        "```\n",
        "\n",
        "### **2.2 Bigram Frequency Counting**\n",
        "We compute bigram counts:\n",
        "```python\n",
        "bigrams = [(tokens[i], tokens[i+1]) for i in range(len(tokens)-1)]\n",
        "bigram_counts = defaultdict(int)\n",
        "for bigram in bigrams:\n",
        "    bigram_counts[bigram] += 1\n",
        "\n",
        "print(\"Bigram counts:\", dict(bigram_counts))\n",
        "```\n",
        "**Output**:\n",
        "```\n",
        "Bigram counts: {\n",
        "  ('the', 'cat'): 2,\n",
        "  ('cat', 'chased'): 1,\n",
        "  ('chased', 'the'): 1,\n",
        "  ('the', 'dog'): 2,\n",
        "  ('dog', 'the'): 1,\n",
        "  ('dog', 'barked'): 1,\n",
        "  ('barked', 'at'): 1,\n",
        "  ('at', 'the'): 1\n",
        "}\n",
        "```\n",
        "\n",
        "### **2.3 Probability Calculation**\n",
        "Compute bigram probabilities:\n",
        "```python\n",
        "unigram_counts = defaultdict(int)\n",
        "for word in tokens:\n",
        "    unigram_counts[word] += 1\n",
        "\n",
        "bigram_probs = {}\n",
        "for bigram, count in bigram_counts.items():\n",
        "    prev_word = bigram[0]\n",
        "    bigram_probs[bigram] = count / unigram_counts[prev_word]\n",
        "\n",
        "print(\"Bigram probabilities:\", bigram_probs)\n",
        "```\n",
        "**Output**:\n",
        "```\n",
        "Bigram probabilities: {\n",
        "  ('the', 'cat'): 0.5,      # P('cat' | 'the') = 2/4\n",
        "  ('cat', 'chased'): 1.0,   # P('chased' | 'cat') = 1/1\n",
        "  ('chased', 'the'): 1.0,   # P('the' | 'chased') = 1/1\n",
        "  ('the', 'dog'): 0.5,      # P('dog' | 'the') = 2/4\n",
        "  ('dog', 'the'): 0.5,      # P('the' | 'dog') = 1/2\n",
        "  ('dog', 'barked'): 0.5,   # P('barked' | 'dog') = 1/2\n",
        "  ('barked', 'at'): 1.0,    # P('at' | 'barked') = 1/1\n",
        "  ('at', 'the'): 1.0        # P('the' | 'at') = 1/1\n",
        "}\n",
        "```\n",
        "\n",
        "### **2.4 Handling Unseen Bigrams (Laplace Smoothing)**\n",
        "If we encounter a new bigram (e.g., `('cat', 'ran')`), Laplace smoothing adjusts probabilities:\n",
        "```python\n",
        "V = len(unigram_counts)  # Vocabulary size = 6\n",
        "\n",
        "def laplace_smoothed_prob(bigram):\n",
        "    return (bigram_counts.get(bigram, 0) + 1) / (unigram_counts.get(bigram[0], 0) + V)\n",
        "\n",
        "print(\"P('ran' | 'cat') with Laplace smoothing:\", laplace_smoothed_prob(('cat', 'ran')))\n",
        "```\n",
        "**Output**:\n",
        "```\n",
        "P('ran' | 'cat') with Laplace smoothing: 0.142857  # ‚âà (0 + 1)/(1 + 6)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Practical Applications & Limitations**\n",
        "\n",
        "### **3.1 Applications**\n",
        "- **Autocomplete**: Predict next word using N-gram probabilities.  \n",
        "- **Spell Correction**: Compare N-gram frequencies (e.g., \"teh\" ‚Üí \"the\").  \n",
        "- **Machine Translation**: Align phrases using N-gram co-occurrences.  \n",
        "\n",
        "### **3.2 Limitations**\n",
        "- **Sparsity**: Many possible N-grams never appear in training.  \n",
        "- **Context Limitation**: Fails to capture long-range dependencies.  \n",
        "- **No Semantics**: Treats \"happy\" and \"joyful\" as unrelated.  \n",
        "\n",
        "---\n",
        "\n",
        "## **4. Extensions & Advanced Topics**\n",
        "- **Interpolation**: Combine multiple N-gram models (e.g., trigram + bigram + unigram).  \n",
        "- **Neural N-grams**: Use embeddings (e.g., Word2Vec) to generalize better.  \n",
        "- **Kneser-Ney Smoothing**: Better handling of unseen N-grams.  \n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "EL7ukTyquQ7U"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b7i9egU-uS8T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}